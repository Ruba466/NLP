{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "23dece31",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"Apple is looking at buying U.K. startup for $1 billion. The deal was finalized in September, and everyone was excited about the acquisition.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b24d283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ebab065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      " ['Apple', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion', '.', 'The', 'deal', 'was', 'finalized', 'in', 'September', ',', 'and', 'everyone', 'was', 'excited', 'about', 'the', 'acquisition', '.']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"Tokens:\\n\",tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c05c6e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens without stop words:\n",
      " ['Apple', 'looking', 'buying', 'U.K.', 'startup', '$', '1', 'billion', 'deal', 'finalized', 'September', 'excited', 'acquisition']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filtered_tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "print(\"Tokens without stop words:\\n\", filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "acd16aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization:\n",
      "Apple  -  Apple\n",
      "is  -  be\n",
      "looking  -  look\n",
      "at  -  at\n",
      "buying  -  buy\n",
      "U.K.  -  U.K.\n",
      "startup  -  startup\n",
      "for  -  for\n",
      "$  -  $\n",
      "1  -  1\n",
      "billion  -  billion\n",
      ".  -  .\n",
      "The  -  the\n",
      "deal  -  deal\n",
      "was  -  be\n",
      "finalized  -  finalize\n",
      "in  -  in\n",
      "September  -  September\n",
      ",  -  ,\n",
      "and  -  and\n",
      "everyone  -  everyone\n",
      "was  -  be\n",
      "excited  -  excited\n",
      "about  -  about\n",
      "the  -  the\n",
      "acquisition  -  acquisition\n",
      ".  -  .\n"
     ]
    }
   ],
   "source": [
    "print(\"Lemmatization:\")\n",
    "for token in doc :\n",
    "    print(token,\" - \",token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4bf89ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text : ['Apple', 'look', 'buy', 'U.K.', 'startup', '$', '1', 'billion', 'deal', 'finalize', 'September', 'excited', 'acquisition']\n"
     ]
    }
   ],
   "source": [
    "cleaned_tokens = []\n",
    "for token in doc :\n",
    "    if not token.is_stop and not token.is_punct and not token.is_space:\n",
    "        cleaned_tokens.append(token.lemma_)\n",
    "\n",
    "print(\"Cleaned Text :\",cleaned_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
